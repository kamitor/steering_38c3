{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üïπÔ∏è Hack your LLM: Modify chatbot behavior with activation steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt a model\n",
    "\n",
    "Huggingface is the main platform for open-weight models. Here's a simple example of how to load and prompt the GPT-2 model by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, convert the prompt string to a list of tokens, the input format for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464, 26175, 32650,   373, 15646,   287,   262,  1748,   286]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The Hamburger was invented in the city of\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'ƒ†Hamb', 'urger', 'ƒ†was', 'ƒ†invented', 'ƒ†in', 'ƒ†the', 'ƒ†city', 'ƒ†of']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize what individual tokens mean\n",
    "tokenizer.convert_ids_to_tokens(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'happy': [\"I just got a promotion at work, and I couldn't be more excited!\", 'The sun is shining, and I feel like today is going to be amazing!', 'I reunited with an old friend, and we laughed for hours reminiscing.', 'My team won the championship, and it feels incredible to celebrate together!', 'The kindness of a stranger made my day unexpectedly beautiful.', 'I finally completed my dream project, and it turned out better than I imagined!', 'The smell of freshly baked cookies always brings a smile to my face.', 'Watching the sunset over the ocean filled me with a sense of peace and joy.', 'Hearing my favorite song on the radio instantly lifted my spirits.', 'Spending time with loved ones reminds me how blessed I am.'], 'sad': ['I just lost my job, and I feel like my world is falling apart.', 'The rain has been pouring all day, mirroring my gloomy mood.', \"Saying goodbye to a loved one is one of the hardest things I've ever done.\", 'The loneliness in my heart feels unbearable right now.', 'I failed at something I worked so hard for, and it hurts deeply.', \"Watching my dreams crumble is a pain I can't describe.\", 'I miss someone so much that it physically aches.', 'The sound of silence in an empty house is deafeningly sad.', \"Realizing I've been forgotten by someone I care about is heartbreaking.\", 'Sometimes, the weight of my emotions feels too heavy to carry.']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Reading a JSON file\n",
    "with open('./happysad.json', 'r') as file:\n",
    "    data = json.load(file)  # Parses JSON into a Python dictionary\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"ƒ†\" is treated as a leading space. Let's generate 100 tokens follwing our prompt with the GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Hamburger was invented in the city of Nuznowsk (in Soviet NKVD archives) in the mid-1940s. It was the first machine that can generate a microwave to produce electricity with which it can cook vegetables at a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = model.generate(**inputs, do_sample=True, max_length=50)\n",
    "\n",
    "# The generated tokens are indices that need to be converted to text\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt injection\n",
    "\n",
    "Let's simply ask the model to answer in the style of a pirate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You are captain Blackbeard who just returned from a long adventure. Speak in a thick dialect. The Hamburger was invented in the city of Longbow's birthplace. It is a very sweet, sweet sandwich. It is also very common in this city\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_injection = \"You are captain Blackbeard who just returned from a long adventure. Speak in a thick dialect. \"\n",
    "\n",
    "prompt_inj = prompt_injection + prompt\n",
    "inputs_inj = tokenizer(prompt_inj, return_tensors=\"pt\")\n",
    "gen_tokens_inj = model.generate(**inputs_inj, do_sample=True, max_length=50)\n",
    "gen_text_inj = tokenizer.batch_decode(gen_tokens_inj)[0] \n",
    "gen_text_inj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. The pirate-style of this message can be better. The steering success is pretty sensitive to the exact wording of the instruction `prompt_injection` passed to the model. We could go down the rabbit hole of prompt engineering at this point. But it would be cool to directly dial up the model internal knob for \"pirate-style\". Luckily, we have full access to the model weigths!\n",
    "\n",
    "Disclamer: There's no guarantee whether this knob exists at all. But recent work in language model interpretability found that many semantic concepts are linearly encoded in activation space ([Park et al.](https://arxiv.org/abs/2311.03658) summarize findings well). Next, we'll try to find a linear \"pirate-direction\" in activation space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing model internals\n",
    "\n",
    "Model inference is a seqence of matrix operations. Let's have a look at the layer structure of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Explainers\n",
    "\n",
    "The neural network architecture of GPT-2 is called a decoder-only Transformer. Callum McDougall created [my favourite explainer of the Transformer architecture](https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch). Another popular ressource is [Jay Alammar's blogpost](https://jalammar.github.io/illustrated-gpt2/). Anthropic's [Mathematical Framework of Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) provides deeper conceptual understanding of the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Caching\n",
    "\n",
    "We'll use the `nnsight` library to access the intermediate results of those matrix opertations. The `nnsight.LanugageModel` class is a wrapper around the `transformers.AutoModelForCausalLM` class we loaded above. Generating text goes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "model_nn = LanguageModel(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Hamburger was invented in the city of Hamburg in 1859. It is a German-style burger with a large, juicy, juicy bun. It is served with a side of lettuce, tomato, and cheese.\\n\\nThe Hamburger is a German-style burger with a large,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with model_nn.generate(prompt, max_new_tokens=50): # The nnsight also takes the prompt string as input and does the tokenization internally\n",
    "    out_tokens = model_nn.generator.output.save()\n",
    "\n",
    "out_text = model_nn.tokenizer.batch_decode(out_tokens)[0]\n",
    "out_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to look for the pirate representation? The localization of concepts in the intermediate layer outputs is an active area of research. Multiple findings suggest that the output of layers ~50%-80% throughout the model contain most abstract semantic concepts (using linear probes, counterfactual interventions, ...). See [this post](https://sidn.baulab.info/stages/#the-remarkable-robustness-of-llms) on different \"stages\" in a Transformer forward-pass.\n",
    "\n",
    "GPT-2 has 12 layers, let's cache the intermediate activation of \"happy\" and \"sad\" at the output of layer 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_8 = model_nn.transformer.h[8] # You can find the name of the layer module in the model diagram above\n",
    "\n",
    "with model_nn.trace(\"happy\"): # NOTE 1: Trace is a single forward pass, no interative, auto-regressive generation. \n",
    "    happy_activation = layer_8.output[0].save() # Confusingly layer_8.output returns a tuple, the activations we want are at idx 0\n",
    "\n",
    "with model_nn.trace(\"sad\"):\n",
    "    sad_activation = layer_8.output[0].save()\n",
    "\n",
    "happy_activation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model internal activations are of shape `[prompt_in_the_batch, token_position, model_dimension]`. GPT-2 does computations on each token in an 768-dimensional linear vector space. The final pirate token representation at layer 8 looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9847e-01, -8.0728e-01, -2.4685e-01, -6.3075e-01,  2.4036e+00,\n",
       "           5.9189e-02,  8.3094e-01, -4.8351e-01, -7.2047e-01, -6.7591e-01,\n",
       "           4.5106e-01,  8.3242e-01, -3.0383e-01, -8.4876e-01, -2.5963e-01,\n",
       "           1.1887e+00,  5.6643e-01, -2.4410e-01,  1.9540e+00,  4.1694e-01,\n",
       "           1.4500e+00, -6.1237e-01, -5.3854e-01,  2.4935e-01,  7.3401e-01,\n",
       "           2.2540e-01,  8.7686e-01,  2.9090e-02, -3.4941e-01, -1.6149e+00,\n",
       "           3.9224e-01,  8.8545e-02,  3.0877e+00, -1.5393e+00,  2.1167e-02,\n",
       "           1.1687e+00,  7.6800e-01, -3.9308e-01, -5.0499e-01, -4.0777e-01,\n",
       "          -4.0968e-01,  1.1589e+00,  1.5200e+00, -1.6700e+00,  5.1582e-01,\n",
       "           2.2641e+00, -1.0502e-01, -8.3348e-01, -3.6577e-01,  2.8220e-01,\n",
       "          -5.8920e-01,  2.5895e-01,  2.7802e+00, -2.5130e-01,  1.8433e-01,\n",
       "           2.4795e+00,  2.4415e+00, -1.1508e+00, -8.0832e-01,  1.1526e+00,\n",
       "          -1.4227e+00,  1.0035e+00, -6.3522e-01,  1.3188e+00,  3.1037e+00,\n",
       "          -2.8989e-01,  1.6270e+00, -2.8449e+00,  6.1308e-01,  1.5948e+00,\n",
       "           9.7796e-02,  2.5333e-01, -7.8779e-01, -1.2169e+00, -1.0206e+00,\n",
       "          -4.2927e-01, -2.3996e+00, -2.9632e-01,  1.0773e+00, -1.2165e-01,\n",
       "          -2.0799e+00, -1.3187e+00, -5.3360e-01,  5.4880e-01, -1.6083e+00,\n",
       "           1.1387e-01, -2.2520e+00, -1.6221e+00,  1.1478e+00, -1.6288e+00,\n",
       "           5.4333e-01, -2.1096e+00, -2.3366e-01,  8.0727e-01, -2.2446e+00,\n",
       "          -1.6004e-02, -6.5225e-01,  2.5127e-01, -1.1292e+00,  1.5085e+00,\n",
       "          -6.8378e-01,  1.5985e+00, -1.8627e+00, -2.4020e+00, -4.6856e-02,\n",
       "           1.2226e+00, -1.1273e+00,  1.0857e+00, -4.9982e-01, -1.1101e+00,\n",
       "          -1.3428e-01, -1.0609e+00, -1.0484e+00,  7.7367e-02, -3.7142e-01,\n",
       "          -4.7774e-01, -2.1314e+00,  8.3835e-01,  3.6602e-01,  7.3778e-01,\n",
       "           3.2085e-01, -6.1934e-01,  5.1155e-01, -7.1832e-01,  5.3824e-01,\n",
       "          -1.0135e+00, -6.0268e-01, -1.0758e+00,  7.3784e-01, -5.9124e-01,\n",
       "           6.7811e-01, -1.8766e-01,  1.3708e+00, -5.5654e-01,  8.2973e-01,\n",
       "           8.2654e-01,  6.1165e-01,  1.3054e-01,  7.8513e+02, -1.9187e-01,\n",
       "          -1.0733e+00,  8.4867e-01, -3.9062e-01, -3.1285e-01,  2.4145e+00,\n",
       "          -3.9817e-02, -1.2266e+00,  1.7373e-01, -3.9270e-01, -1.1130e+00,\n",
       "          -2.4362e-01, -9.0085e-01, -3.1188e-01,  2.0063e-02, -1.3673e+00,\n",
       "           4.9878e-01, -4.4341e-02, -2.8328e+00,  9.6761e-01,  4.2797e-01,\n",
       "          -1.0851e+00,  1.6795e+00,  7.7054e-02, -6.7768e-01, -2.3018e+00,\n",
       "          -9.7685e-01,  2.4904e-01,  4.9839e-01,  4.4640e-01, -3.5852e-01,\n",
       "          -3.3300e-02,  2.9285e-01, -1.8067e-01, -1.0309e+00,  8.6697e-01,\n",
       "           4.1205e-01, -3.7717e-01, -1.0137e+00,  2.4567e+00, -1.1622e-01,\n",
       "          -9.2864e-01,  4.6014e-01, -1.7478e+00, -8.4718e-01,  7.5441e-03,\n",
       "          -7.7873e-02, -1.8597e-01,  3.9866e-01,  4.4058e-01,  2.0455e-01,\n",
       "           3.2886e-01,  5.1782e-01, -2.4175e+00,  2.3966e-01, -1.2434e+00,\n",
       "           1.6929e+00,  7.6160e-01, -1.2478e+00, -5.9528e-02, -8.3978e-01,\n",
       "           1.7853e-01,  2.1216e+00, -1.4308e+00, -2.6873e-01,  6.2322e-01,\n",
       "          -6.4182e-01, -6.8735e-01,  3.1805e-01, -2.0673e+00,  1.1514e+00,\n",
       "           1.1751e+00,  1.0001e+00,  6.8969e-01, -4.5041e-01, -4.3399e-01,\n",
       "          -5.4649e-01, -9.8510e-01, -1.1173e+00, -1.4287e+00,  2.3733e-02,\n",
       "          -4.0166e-01, -5.5982e-01,  1.5471e+00, -1.5462e+00,  3.9528e-01,\n",
       "          -6.7342e-01, -2.1308e-01,  4.2986e-01, -1.4682e+00,  1.2321e+00,\n",
       "           1.1052e+00,  1.3185e+00,  1.7665e+00, -8.2469e-01, -9.5749e-01,\n",
       "          -2.9294e+00, -1.1073e+00,  1.1251e+00,  9.5723e-01, -4.2692e-01,\n",
       "           1.1039e+00, -8.1944e-01,  3.6336e-01,  7.9052e-01, -4.0721e-02,\n",
       "          -2.4962e-01, -1.0844e+00, -2.8862e-01,  2.7902e-01,  8.7180e-01,\n",
       "           7.6171e-01,  6.0894e-01, -4.0801e-01,  1.9251e+00, -1.1341e+00,\n",
       "           2.9996e-01,  1.1393e+00,  3.7054e-01, -4.4494e-01, -7.7132e-02,\n",
       "          -9.4542e-01, -4.5942e-01,  7.2327e-01,  1.1097e+00, -3.7616e-01,\n",
       "          -1.4732e+00, -1.3262e+00,  1.5851e+00, -1.4465e-01,  4.8031e-01,\n",
       "           2.1694e+00,  1.6265e+00, -1.2790e+00,  1.7271e+00, -1.2667e+00,\n",
       "           2.6611e-01,  2.9122e+00,  1.3576e-02,  5.9734e-01, -4.9862e-01,\n",
       "           1.6584e-01,  6.5375e-01, -1.9485e-01,  3.6248e-01,  9.5612e-01,\n",
       "          -2.0346e+00, -2.5779e-01, -1.3051e-01,  2.0214e+00,  4.6068e-01,\n",
       "          -1.3338e-01, -7.6593e-01,  2.5477e-01, -4.2706e-01,  1.0319e+00,\n",
       "          -1.9253e+00, -1.1271e+00, -1.7647e+00, -2.1407e-01, -4.2837e-01,\n",
       "           4.1240e-01,  6.6805e-01,  1.1923e+00,  5.1572e-01,  5.6685e-01,\n",
       "           1.0472e+00, -6.2814e-01,  9.8095e-01, -1.4994e+00,  2.0659e-01,\n",
       "           8.9541e-01,  1.3502e+00, -2.0525e+00,  5.9410e-01, -1.9032e+00,\n",
       "           2.3064e-01, -1.6532e+00,  5.6106e-01, -9.8088e-02,  1.8559e+00,\n",
       "          -1.4996e+00, -1.7472e-01,  8.1120e-02,  1.7338e-01,  1.1515e+00,\n",
       "          -1.7988e+00,  4.7295e+00,  5.7756e-01,  8.1085e-01, -6.0799e-01,\n",
       "          -9.4370e-01, -2.7084e-01,  1.5438e+00, -2.1575e-01,  1.5115e+00,\n",
       "          -1.2180e+00,  7.1636e-01, -4.8352e-01,  2.6906e-02, -4.3969e-02,\n",
       "           1.0368e+00, -1.3290e+00,  9.2578e-01, -6.6717e-01, -5.7873e-01,\n",
       "           1.4113e+00, -8.1368e-01, -1.2537e+00,  5.8304e-01,  1.7555e-01,\n",
       "          -2.2688e+00,  5.9533e-01,  7.8763e-01,  4.0160e-01,  1.3108e+00,\n",
       "          -2.5061e-01, -2.3098e+00, -1.0037e-02, -5.0197e-01,  2.2774e-01,\n",
       "          -4.5095e-01, -1.9547e-01, -6.3273e-03, -1.5733e+00, -8.2368e-01,\n",
       "          -1.4310e+00, -5.7919e-01, -3.0346e-02,  2.4492e+00,  8.8805e-01,\n",
       "           4.4813e-01,  1.5212e-01,  1.7726e-01, -1.1686e+01, -2.8247e+00,\n",
       "          -4.0687e-01, -1.3386e-01, -1.5895e-02, -4.6502e+01, -2.3367e-01,\n",
       "          -3.8762e-01,  1.1072e+00, -1.6035e-01, -9.9903e-01, -1.4396e+00,\n",
       "           1.1229e-01, -3.4751e-01,  2.0447e-01, -8.2819e-01, -3.8356e-01,\n",
       "           2.6114e+00, -7.3402e-01,  1.1932e+00, -2.2255e-01,  5.4695e-01,\n",
       "           9.6063e-01, -1.6989e+00,  1.3923e-01, -1.0810e+00, -7.4876e-01,\n",
       "           1.0624e+00, -5.9981e-01, -9.7004e-01,  9.7241e-01, -3.9769e-01,\n",
       "          -3.8606e-01,  1.0007e-01, -2.1382e-01,  7.2632e-01,  8.0327e-02,\n",
       "          -9.4880e-01,  1.5490e-01, -9.4114e-01, -5.5385e-01, -8.9398e-01,\n",
       "           6.2873e-01,  1.4540e+00,  1.2122e-01, -8.3314e-01,  1.1316e-01,\n",
       "          -7.4701e-01, -3.4750e-01,  9.9822e-01,  9.9012e-01,  1.4979e-01,\n",
       "          -1.0102e-01, -1.2720e+00,  8.0954e-01,  1.1447e+00,  5.0016e-01,\n",
       "           6.6787e-01,  8.1140e-01,  1.4339e+00,  9.6283e-01, -1.0059e+00,\n",
       "           1.4014e-01,  2.3625e-01,  1.7428e-01,  1.4631e+00,  4.9337e-01,\n",
       "          -4.6383e-01,  1.4178e+00, -4.9101e-02,  1.5222e+00, -1.9261e-01,\n",
       "           7.3319e-01,  3.1520e-01,  3.0043e+03, -5.8338e-01,  8.7059e-01,\n",
       "          -8.9572e-01,  3.8908e-02,  7.3390e-01,  7.4514e-01, -8.4292e-01,\n",
       "          -1.9666e+00,  7.4283e-01,  8.2085e-02, -6.7971e-01, -1.0240e-01,\n",
       "           7.9800e-01,  5.7915e-01, -2.3929e-01, -1.1431e+00,  2.5434e+00,\n",
       "          -1.1165e+00,  4.0638e-01, -2.7982e-01,  7.3807e-01,  7.5285e-01,\n",
       "           1.7384e-01, -1.0751e+00, -3.4630e-02, -3.2520e-01, -9.4127e-03,\n",
       "          -5.6076e-01, -1.4076e+00, -1.0354e+00, -2.1784e-01, -4.8274e-01,\n",
       "           7.4275e-02,  2.2167e+01,  2.5937e-01,  7.4911e-01,  1.3737e+00,\n",
       "           8.8296e-01,  9.5383e-01,  5.2687e-01,  4.6195e-01, -5.5147e-01,\n",
       "           6.5014e-01,  2.6436e-01,  1.0208e+00,  6.5668e-01, -4.5083e-01,\n",
       "          -5.5140e-02,  4.0316e+00, -9.4709e-01,  1.0349e+00,  5.3590e-02,\n",
       "           1.6631e+00,  5.8149e-01,  9.0979e-01, -2.5417e+00,  1.5787e-01,\n",
       "          -1.3814e+00,  1.1608e-01,  4.1844e-01, -3.4485e-01,  9.0224e-01,\n",
       "           2.9246e-01,  1.0310e+00, -1.5072e+00,  7.7320e-02,  1.2660e-01,\n",
       "          -5.9039e-01, -3.7486e-01, -5.9205e-01, -7.5638e-01, -4.3990e-01,\n",
       "           4.0158e-02,  4.6286e-01,  1.7425e-01,  5.4973e-01, -4.8407e-01,\n",
       "          -1.8908e-01,  5.3062e-03, -2.6130e+00, -1.4606e+00, -9.7898e-01,\n",
       "          -6.7460e-01,  6.1511e-02,  2.6976e-01,  2.8571e-01, -5.0860e-01,\n",
       "          -3.8379e-02,  4.8165e-01, -9.2015e-01, -1.9579e-01, -1.8552e+00,\n",
       "           3.8693e-01, -1.1174e+00, -5.9689e-02,  1.0307e+00, -4.9382e-01,\n",
       "          -2.1201e+00,  1.9485e-01, -5.4112e-01,  1.6316e+00, -2.1281e+00,\n",
       "          -2.6322e-01, -8.0349e-01,  6.8495e-01, -5.8046e-01, -1.3361e+00,\n",
       "           5.9017e-02, -4.9835e-02,  8.5087e-01, -1.6359e+00, -8.0238e-01,\n",
       "           1.0852e-01,  1.2108e+00, -1.6735e+00,  4.7129e-03,  2.8847e-01,\n",
       "          -4.4406e-01, -1.2821e+00, -1.6970e+00,  1.1681e+00, -1.0894e+00,\n",
       "          -2.1449e+00,  9.0584e-01,  1.1528e+00, -1.2523e+00,  4.6452e-01,\n",
       "          -1.2031e-01,  5.2936e-01,  7.7322e-01,  2.4231e-01,  1.6667e+00,\n",
       "           8.9332e-01,  2.9013e-01,  1.5867e-01,  2.2415e-01, -6.3050e-01,\n",
       "           3.2424e-01,  1.2423e-01, -1.2937e+00,  6.3150e-02, -9.4093e-01,\n",
       "           2.2101e+00,  3.0296e-01, -6.2601e-01, -5.0893e-02, -5.4290e-01,\n",
       "           1.0052e+00, -5.4427e-02,  1.1393e+00, -5.2707e-01, -9.6179e-02,\n",
       "           7.2186e-01,  4.9837e-01,  6.7195e-01, -9.8532e-01, -7.8810e-01,\n",
       "          -3.1325e-01,  3.3963e-01,  1.1029e-01, -6.4177e-01,  1.0088e+00,\n",
       "           3.1948e-01, -9.0469e-01,  2.7708e-01,  5.8241e-01, -1.0878e+00,\n",
       "          -7.2034e-01,  5.3544e-01, -2.5800e-02, -2.2524e-01, -1.0029e+00,\n",
       "          -4.4411e-01,  2.3254e+00,  5.9418e-01,  8.7853e-01,  7.7733e-01,\n",
       "           3.1475e-02, -2.0990e+00, -2.9536e+00,  9.6677e-01, -3.4367e-01,\n",
       "           9.6730e-01, -1.1544e+00,  6.9861e-01, -6.6762e-01, -6.2473e-02,\n",
       "           4.1891e-02, -4.3137e-01, -8.9864e-01,  1.4353e-01, -6.1060e-01,\n",
       "           1.1896e-01,  1.1935e-01, -1.7024e+00,  1.8639e+00,  1.4900e-01,\n",
       "          -1.8034e-01, -1.0784e+00, -1.9407e+00, -1.0324e+00,  6.4917e-01,\n",
       "          -4.9144e-01, -1.9157e-02,  1.9638e+00, -1.0671e+00,  1.0956e+00,\n",
       "          -1.0192e+00,  1.3114e-01,  7.0531e-01, -8.7873e-01, -2.1515e+00,\n",
       "           3.0726e-01, -1.1193e+00, -8.8062e-01,  1.0625e-01, -2.0749e+00,\n",
       "          -1.1923e+00,  3.7187e-01, -1.2998e+00, -7.9092e-01, -9.3287e-03,\n",
       "           3.8463e+00, -2.7636e-01,  2.5181e-01,  8.1083e-01, -1.4489e+01,\n",
       "           1.3619e+00, -6.4468e-01, -5.1003e-01, -6.2345e-01, -2.2326e+00,\n",
       "          -7.6718e-01,  8.0574e-01, -8.4311e-01, -4.3088e-01,  1.3449e-01,\n",
       "          -6.3663e-01,  1.1364e+00, -2.1870e-03,  1.1866e+00, -6.0039e-01,\n",
       "          -1.6936e+00,  1.0136e-01,  5.2166e-01,  9.7502e-01, -2.4386e-01,\n",
       "           8.1803e-01,  3.4226e-01, -7.8205e-01,  1.1937e+00, -1.0410e+00,\n",
       "           9.8589e-01, -1.1310e+00,  1.2782e+00, -1.0933e+00, -8.7403e-01,\n",
       "          -1.0594e+00, -1.5358e+00, -4.4010e-01, -1.7877e+00,  1.7437e+00,\n",
       "           4.2254e-01,  4.3061e-01, -3.5364e-01, -6.4066e-01, -5.7918e-01,\n",
       "          -3.5527e-01,  1.9461e-01,  4.5141e-01, -3.0369e+00, -1.1653e-01,\n",
       "           1.7943e+00,  1.4505e+00,  4.3924e-01,  1.7098e-01,  2.5887e+00,\n",
       "          -3.5357e-01,  1.8678e+00,  3.1286e-02,  1.9765e+00, -3.3031e-01,\n",
       "           7.9643e-01, -9.8360e-01, -1.1306e+00, -9.7765e-02, -9.6203e-01,\n",
       "          -6.1311e-01,  1.6918e-01,  5.7619e-01, -2.9938e-01,  8.7240e-02,\n",
       "           4.6908e-01, -7.3586e-01, -1.2003e+00,  4.3694e-01, -4.8257e-01,\n",
       "          -2.9115e+00, -1.7816e+00, -2.6572e-01, -1.2994e+00, -4.6986e-01,\n",
       "          -7.8563e-02, -4.4883e-01,  8.0903e-01, -9.8825e-01, -5.1900e-01,\n",
       "          -5.9062e-01, -6.5506e+00, -1.7150e+00, -1.7176e+00,  1.0365e-01,\n",
       "          -1.0932e+00, -1.3473e-01,  5.5632e-01, -9.6468e-03,  3.5778e-01,\n",
       "          -1.3210e+00, -6.2758e-01, -3.8306e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering with activation addition\n",
    "\n",
    "Let's add this representation with an (arbitrarily chosen) factor of 2 to the final token of our original prompt, where the prediction for the next token is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be steering with contrastive activation addition\n",
    "act_diff = happy_activation[0, -1, :] - sad_activation[0, -1, :]\n",
    "steering_factor = 0.01\n",
    "steering_vector = steering_factor * act_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (TypeError('__index__ returned non-int (type InterventionProxy)')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Hamburger was invented in the city of Hamburg in 1859. It is a German-style burger, with a large, round, and slightly round meat. It is served with a large, round, and slightly round meat.\\n\\nThe Hamburger is a German-style burger,']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_generated_tokens = 50\n",
    "\n",
    "with model_nn.generate(prompt, max_new_tokens=50):\n",
    "    layer_8 = model_nn.transformer.h[8] # Reinitialize the layer object\n",
    "\n",
    "    for _ in range(num_generated_tokens):\n",
    "        layer_8_out = layer_8.next().output # Cache the current activaiton, tuple\n",
    "        layer_8_acts = layer_8_out[0]\n",
    "        layer_8_acts[:, -1] += steering_vector # Modify\n",
    "        layer_8.output = (layer_8_acts,) + layer_8_out[1:] # Update the layer with the modified activations\n",
    "        # model.next()\n",
    "\n",
    "    out_tokens = model_nn.generator.output.save()\n",
    "\n",
    "out_text = model_nn.tokenizer.batch_decode(out_tokens)\n",
    "out_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "[Neuronpedia](https://www.neuronpedia.org/gemma-2-9b-it/steer)\n",
    "\n",
    "[Transluce Monitor](https://monitor.transluce.org/dashboard/chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Demos\n",
    "\n",
    "- [Steering Tutorial in the ARENA program by Callum McDougall](https://arena3-chapter1-transformer-interp.streamlit.app/[1.4.2]_Function_Vectors_&_Model_Steering)\n",
    "- [Steering Tutorial SAELens by Decode Research](https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
